{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Documentation for CLARA","text":"<p>CLARA is short for CLuster Architecture Recovery Assistent. CLARA can help by extracting component-based software architectures from applications deployed in Kubernetes clusters and exporting them into a visually appealing format. Thus, IT-architects that need insights about the components actually deployed in the Cluster can be assisted by using CLARA.</p> <p>For details on the functionality of CLARA see the concept page. For information on configuration options of CLARA see the configurations page. For information on the validation of CLARA's functionality see the validation page for the T2 Reference Architecture.</p> <p>CLARA is open-sourced under the MIT License.</p>"},{"location":"aggregation/","title":"Aggregation","text":"<p>For aggregating data from the cluster four different data sources are used in CLARA:</p> <ul> <li>Kubernetes API</li> <li>Kubernetes DNS-Server</li> <li>OpenTelemetry Traces</li> <li>anchore/syft</li> </ul> <p>Aggregations can be enabled/disabled, yet it is recommended to always use all available aggregators to get a holistic view of the examined architecture. For details see configurations.</p>"},{"location":"aggregation/platforms/kubernetes/api/","title":"API","text":"<p>CLARA utilizes the Kubernetes API to retrieve basic information about the pods, services and deployments running in the cluster.</p> <p>The fabric8 Kubernetes client is used by CLARA to communicate to the Kubernetes API.</p>"},{"location":"aggregation/platforms/kubernetes/api/#concept","title":"Concept","text":"<p>The Kubernetes API is queried for pods and services from the configured namespace (see configurations). Pods are then matched to a service and all services with their respective pods and all unmatched pods are provided into the datapipeline.</p>"},{"location":"aggregation/platforms/kubernetes/dns/","title":"DNS","text":"<p>CLARA can analyze the logs of CoreDNS (the default Kubernetes DNS server) to discover communication of components via DNS queries. For that feature to work correctly, it is crucial that the DNS server is configured to log DNS queries by enabling the <code>log</code> plugin.</p> An example ConfigMap for CoreDNS with the 'log' plugin enabled<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: coredns\n  namespace: kube-system\ndata:\n  Corefile: |\n    .:53 {\n        log\n        errors\n        health\n        ready\n        kubernetes cluster.local in-addr.arpa ip6.arpa {\n          pods insecure\n          fallthrough in-addr.arpa ip6.arpa\n        }\n        prometheus :9153\n        forward . /etc/resolv.conf\n        cache 30\n        loop\n        reload\n        loadbalance\n    }\n</code></pre> <p>Other DNS servers</p> <p>Your cluster might come with additional DNS servers to reduce the load. A prominent example is the node-local-dns for caching DNS. There, you must also enable the <code>log</code> plugin.</p> <p>Compatible DNS servers</p> <p>Because CLARA analyzes the logged DNS queries,</p> <ol> <li>query logging must be activated</li> <li>the query logs must be compatible with the CoreDNS logs.</li> </ol> <p>Currently, CLARA analyzes all logs from the pods with the labels <code>k8s-app=kube-dns</code> or <code>k8s-app=node-local-dns</code> in the namespace <code>kube-system</code>.</p>"},{"location":"aggregation/platforms/kubernetes/dns/#managed-kubernetes-cluster","title":"Managed Kubernetes cluster","text":"<p>Using a managed Kubernetes cluster from a service provider</p> <p>When using a managed cluster from a service provider, changes to core components of Kubernetes might be not allowed directly. Please consult the documentation of your respective provider.</p>"},{"location":"aggregation/platforms/kubernetes/dns/#digitalocean","title":"DigitalOcean","text":"<p>For DigitalOcean, the correct way of enabling logging is to create a special ConfigMap:</p> ConfigMap to activate query logging for CoreDNS in a Kubernetes cluster managed by DigitalOcean<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: coredns-custom\n  namespace: kube-system\ndata:\n  log.override: |\n    log\n</code></pre>"},{"location":"aggregation/platforms/kubernetes/dns/#dns-debugging","title":"DNS debugging","text":"<p>As described in the Kubernetes Documentation, you can use dnsutils to debug DNS resolution. For CLARA, this is also a simple way of creating DNS queries explicitly and checking if CLARA detects the communication. Just create a dnsutils-pod with the following manifest:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: dnsutils\n  namespace: default\nspec:\n  containers:\n    - name: dnsutils\n      image: registry.k8s.io/e2e-test-images/jessie-dnsutils:1.7\n      command:\n        - sleep\n        - \"infinity\"\n      imagePullPolicy: IfNotPresent\n  restartPolicy: Always\n</code></pre> <p>Then you can use the following command to execute DNS queries:</p> <pre><code>kubectl exec -it dnsutils -n default -- nslookup google.com\n</code></pre> <p>Execute the following command to check the DNS server logs:</p> <pre><code>kubectl logs -l k8s-app=kube-dns -n kube-system\n</code></pre>"},{"location":"aggregation/platforms/kubernetes/dns/#concept","title":"Concept","text":"<p>The log DNS analysis uses the obtained information from the Kubernetes API to match the hostnames and ip-addresses in a DNS log to components of the cluster. An example log can look like this and provides disclosure about the source and target of a communication. <pre><code>[INFO] 10.244.0.19:35065 - 3179 \"A IN kubernetes.default.svc.cluster.local.svc.cluster.local. udp 72 false 512\" NXDOMAIN qr,aa,rd 165 0.0000838s\n</code></pre></p>"},{"location":"aggregation/platforms/kubernetes/opentelemetry/","title":"OpenTelemetry","text":"<p>CLARA can utilize OpenTelemetry traces as data source for finding components and to some extent component types, as well as communications between components. For that feature to work correctly, it is crucial to have instrumented applications and an OpenTelemetry Collector running in the cluster as described below.</p>"},{"location":"aggregation/platforms/kubernetes/opentelemetry/#concept","title":"Concept","text":"<p>\"OpenTelemetry is an Observability framework and toolkit designed to create and manage telemetry data such as traces, metrics, and logs.\"  Traces and metrics are generated by each component individually and are forwarded to an OpenTelemetry collector, which processes the telemetry data and distributes it to a backend which utilizes it. CLARA can be seen as such a backend, which offers a gRPC endpoint for the oTel-collector to forward the traces to. CLARA then iterates over the traces and extracts information about components and their communications from that.</p>"},{"location":"aggregation/platforms/kubernetes/opentelemetry/#setup","title":"Setup","text":"<p>When using OpenTelemetry for CLARA you first need to ensure your software components are instrumented with OpenTelemetry traces. If not consider using OpenTelemetry auto-instrumentation.</p> <p>OpenTelemetry Semantic Conventions</p> <p>Because OpenTelemetry traces' attributes are not standardized, it is recommended to use tracing with the OpenTelemetry semantic conventions for CLARA. If your services do not provide them, you can try to set up the OpenTelemetry auto-instrumentation on top of your system.</p> <p>Second, ensure that there is an OpenTelemetry collector with the matching configuration is running in your cluster. Third, when you use CLARA on a local machine and do not deploy in the cluster, you need to forward the traces from the OpenTelemetry collector to your local machine. The open-source tool ktunnel can be used to achieve this.</p>"},{"location":"aggregation/platforms/kubernetes/opentelemetry/#opentelemetry-collector","title":"OpenTelemetry Collector","text":"<p>The OpenTelemetry collector is a default component provided by OpenTelemetry itself. For CLARA only traces are used, thus the minimal configuration  The image can be used to deploy a container with a suitable configuration as shown below. Examples for service and deployment configurations can be found in clara/deployment/open-telemetry-collector/deployment.yml</p> An example ConfigMap for the oTel-collector deployment<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: otel-collector-conf\n  labels:\n    app: otel-collector-conf\n    component: otel-collector-conf\ndata:\n  otel-collector-conf: |\n    receivers:\n      otlp:\n        protocols:\n          grpc:\n          http:\n\n    processors:\n\n    exporters:\n      otlp:\n        endpoint: \"localhost:7878\"\n        tls:\n          insecure: true\n\n    service:\n      pipelines:\n        traces:\n          receivers: [otlp]\n          processors: []\n          exporters: [otlp]\n</code></pre>"},{"location":"aggregation/platforms/kubernetes/opentelemetry/#ktunnel","title":"ktunnel","text":"<p>ktunnel is an open-source tool that enables reverse port-forwarding to extract data out of kubernetes clusters. In order to use CLARA on a local machine, a ktunnel sidecar can be attached towards the OpenTelemetry collector deployment using  <code>ktunnel inject deployment otel-collector-deployment 7878 -n &lt;namespace&gt;</code> For further information see the ktunnel docs. </p>"},{"location":"aggregation/platforms/kubernetes/opentelemetry/#opentelemetry-auto-instrumentation","title":"OpenTelemetry Auto-instrumentation","text":"<p>OpenTelemetry Auto-instrumentation can be used to generate OpenTelemetry traces on software components that are not instrumented themselves in Kubernetes clusters. This works by applying sidecar containers to each yet to be instrumented service that capture the network traffic and generate traces from that. For documentation on installation please see the official docs from OpenTelemetry.</p>"},{"location":"aggregation/platforms/kubernetes/opentelemetry/#aggregation-algorithms","title":"Aggregation Algorithms","text":"<p>After the OpenTelemetry-aggregator finished collecting the traces, the algorithm will iterate over all traces and extract architectural information from it. An OpenTelemetry spans can be of one of five kinds, Producer, Consumer, Client, Server and Internal. Internal spans are ignored, Client- and Server-spans as well as Producer- and Consumer- spans are analyzed seperated from each other, as described below.</p>"},{"location":"aggregation/platforms/kubernetes/opentelemetry/#client-server","title":"Client-Server","text":"<p>Spans of Client and Server kind are analyzed the following way:</p> <p>A client as well as a server span can disclose information about the sending component as well as the respective other component in the communication. Therefore, from each span two possible components are obtained, that receive all the information available.</p> <p>As there is no definitive standard for the naming and the values of span-attributes, the following logic is applied to extract information from span attributes:</p> <ul> <li>For each seeked information (hostname, port, ip-address, and path) of the server and the client a list of often used key names is provided (e.g. server.address, url.path, etc.). </li> <li>The spans attributes are then filtered for those key-names and if they match, regexes are applied in order to find the specific attribute.</li> <li>Two component-objects are created and with all available information and simply added to a list of found components.</li> <li>A relation (communication) object with the client-name and if available the server-name, otherwise the server hostname or ip-address is added.</li> </ul>"},{"location":"aggregation/platforms/kubernetes/opentelemetry/#producer-consumer","title":"Producer-Consumer","text":"<p>Producers and Consumers of message-oriented communications are specifically tagged, because there is no directly observable communication between the source and the target component. Based on the semantic conventions, however, a producer-span contains the messaging-destination which can be used to obtain the communication. Therefore, the recovery looks as follows:</p> <ul> <li>Three components are created, the source, the target and the message-broker </li> <li>A Messaging relation (communications) containing the source, the target and the messaging system. </li> </ul>"},{"location":"aggregation/platforms/kubernetes/opentelemetry/#merging","title":"Merging","text":"<p>As each analyzed span creates at least two component objects, those need to be merged into a consistent pattern. The merging is done the following way:</p> <ul> <li>All component objects without a service name are tried to be matched to component having a service name via the hostname, the ip-address or the endpoint-list.</li> <li>Components that can't be matched will be dealt with afterward.</li> </ul>"},{"location":"aggregation/platforms/kubernetes/opentelemetry/#mapping","title":"Mapping","text":"<p>The component objects finally need to be mapped to the CLARA-wide internal component and communication representation.  Component objects containing a service-name are mapped to an \"internal\" component object, components without a service-name are mapped to an \"external\" one. Communications are mapped if a matching component via service-name or hostname for source and target can be found.</p>"},{"location":"aggregation/platforms/kubernetes/syft/","title":"SBOM","text":"<p>CLARA can utilize anchore/syft to create SBOM files in SPDX format from the recovered components. This is done to extract the dependencies and external libraries of the recovered architecture.</p>"},{"location":"aggregation/platforms/kubernetes/syft/#concept","title":"Concept","text":"<p>In order to get the library information of each component, CLARA passes the recovered image and version tag to syft. The syft binary then fetches the image from docker-hub and analyzes its contents and creates the SPDX files. Lastly, the SPDX files for the components are read by CLARA and each library and version from is added to the respective component.</p>"},{"location":"aggregation/platforms/kubernetes/syft/#setup","title":"Setup","text":"<p>Install the binary from anchore/syft for your respective OS: macOS: <pre><code>brew install syft\n</code></pre> All OS: <pre><code>curl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh | sh -s -- -b /usr/local/bin \n</code></pre> For configuration options please see the configurations page.</p>"},{"location":"concept/","title":"Concept","text":"<p>CLARA itself is a data pipeline that collects information about an application deployed in a Kubernetes cluster from different data sources (i.e. the Kubernetes API, the Kubernetes internal DNS server and OpenTelemetry traces), merges them, filters them and exports them to visually display the architecture of the examined application.</p>"},{"location":"concept/#datapipeline","title":"Datapipeline","text":"<p>The datapipeline of CLARA consists of the four main steps:</p> <ul> <li>Aggregation </li> <li>Merging </li> <li>Filtering </li> <li>Export </li> </ul> <p></p>"},{"location":"configuration/","title":"Configuration","text":"<p>The typical configuration for CLARA is in the YAML format. Below, each available option is explained. All options with a default value are optional.</p> <p>Sensitive Information &amp; Environment Variables</p> <p>Sensitive information, like usernames and passwords, don't belong into configuration files! For that reason, each configuration option can be specified by an environment variable. Instead of the actual value, the BASH-like syntax <code>${NAME_OF_THE_ENV_VARIABLE}</code> can be used to specify the environment variable.</p> <p>Interpolation and specifying default values is possible as well, just have a look here! There you can also find other ways to set up the configuration to be effective, like referencing and substituting other parts of the configuration to make it more DRY.</p>"},{"location":"configuration/#general-configuration-options","title":"General configuration options","text":"app.log-config <ul> <li>Type: Boolean</li> <li>Default: false</li> <li>Description: Whether the whole configuration that CLARA will use should be logged at startup (at the info-level).</li> </ul> app.block-after-finish <ul> <li>Type: Boolean</li> <li>Default: false</li> <li>Description: Whether the CLARA process should keep running after all exports are finished.   This is useful when CLARA itself is running e.g. in Kubernetes and the Pod should not terminate because of the automatic restarts.</li> </ul>"},{"location":"configuration/#configuring-the-aggregation","title":"Configuring the aggregation","text":""},{"location":"configuration/#platform-kubernetes","title":"Platform: Kubernetes","text":"aggregation.platforms.kubernetes.include-kube-namespaces <ul> <li>Type: Boolean</li> <li>Default: false</li> <li>Description: Whether the namespaces with the <code>kube</code>-prefix should also get scanned by CLARA.   Must be set to true when every namespace should be scanned, even when namespaces has the <code>*</code>-wildcard.</li> </ul> aggregation.platforms.kubernetes.namespaces <ul> <li>Type: List of Strings</li> <li>Default: empty List</li> <li>Description: List all namespaces which CLARA should scan.   To just scan all namespaces (except the <code>kube</code>-namespaces) set just the <code>*</code>-wildcard as the only element.   The <code>*</code> needs to be in quotes.</li> </ul>"},{"location":"configuration/#aggregator-kubernetes-api-optional","title":"Aggregator: Kubernetes API (optional)","text":"aggregation.platforms.kubernetes.aggregators.kube-api.enable <ul> <li>Type: Boolean</li> <li>Default: true</li> <li>Description: Simple way to disable this aggregator without removing all of its associated configuration.</li> </ul>"},{"location":"configuration/#aggregator-dns-optional","title":"Aggregator: DNS (optional)","text":"aggregation.platforms.kubernetes.aggregators.dns.enable <ul> <li>Type: Boolean</li> <li>Default: true</li> <li>Description: Simple way to disable this aggregator without removing all of its associated configuration.</li> </ul> aggregation.platforms.kubernetes.aggregators.dns.logs-since-time <ul> <li>Type: String (formatted as RFC3339, like <code>2024-01-01T00:00:00Z</code>)</li> <li>Default: empty String</li> <li>Description: The DNS aggregator works by analyzing the logs of the Kubernetes DNS server which must be configured to log the queries.   This option defines how recent the logs must be to be considered by CLARA.   If this option is just an empty String (the default), all available logs will be used, which can lead to unwanted side effects, like old logs from a previous version of the deployment polluting the recovered architecture.</li> </ul> aggregation.platforms.kubernetes.aggregators.dns.use-logs-from-file <ul> <li>Type: Boolean</li> <li>Default: false</li> <li>Description: Simple way to switch if the DNS logs should be read from a file instead the Kubernetes API directly.</li> </ul> aggregation.platforms.kubernetes.aggregators.dns.path-to-dns-logs <ul> <li>Type: String</li> <li>Default: empty String</li> <li>Description: The DNS aggregator when used with files instead of the API must obtain an absolute path to the file.</li> </ul>"},{"location":"configuration/#aggregator-opentelemetry-optional","title":"Aggregator: OpenTelemetry (optional)","text":"aggregation.platforms.kubernetes.aggregators.open-telemetry.enable <ul> <li>Type: Boolean</li> <li>Default: true</li> <li>Description: Simple way to disable this aggregator without removing all of its associated configuration.</li> </ul> aggregation.platforms.kubernetes.aggregators.open-telemetry.listen-port <ul> <li>Type: Integer (must be a valid port number)</li> <li>Description: The port CLARA will listen to incoming spans sent by an OpenTelemetry collector.</li> </ul> aggregation.platforms.kubernetes.aggregators.open-telemetry.listen-duration <ul> <li>Type: String (format here)</li> <li>Description: Amount of time CLARA should listen to incoming spans sent by an OpenTelemetry collector.</li> </ul>"},{"location":"configuration/#aggregator-syft-sbom-optional","title":"Aggregator: syft-sbom (optional)","text":"aggregation.platforms.kubernetes.aggregators.syft-sbom.enable <ul> <li>Type: Boolean</li> <li>Default: true</li> <li>Description: Simple way to disable this aggregator without removing all of its associated configuration.</li> </ul> aggregation.platforms.kubernetes.aggregators.syft-sbom.sbom-file-path <ul> <li>Type: String (a valid relative or absolute path e.g. \"sbom/\")</li> <li>Description: The path where CLARA stores the generated SPDX-JSON files.</li> </ul> aggregation.platforms.kubernetes.aggregators.syft-sbom.use-stored-sbom-files <ul> <li>Type: Boolean</li> <li>Default: false</li> <li>Description: States if the SPDX-JSON files for aggregating the libraries should be newly generated or not.</li> </ul>"},{"location":"configuration/#configuring-the-merge","title":"Configuring the merge","text":"merge.comparison-strategy <ul> <li>Type: String (one of <code>Equals</code>, <code>Prefix</code>, <code>Suffix</code>, <code>Contains</code>)</li> <li>Description: Strategy for matching names of components aggregated by different aggregators.   <code>Equals</code> needs the same names, <code>Prefix</code> and <code>Suffix</code> need to have matching strings on the start or the end respectively, <code>Contains</code> needs that one string is part of the other.</li> </ul> merge.show-messaging-communications-directly <ul> <li>Type: Boolean</li> <li>Description: If <code>true</code>, CLARA will define communications that go via a message broker directly between the components and removes the communications to the message broker. If <code>false</code> it show the communications via the message broker. </li> </ul>"},{"location":"configuration/#configuring-the-filter","title":"Configuring the filter","text":"filter.remove-component-endpoints <ul> <li>Type: Boolean</li> <li>Default: false</li> <li>Description: If <code>true</code>, the endpoints of the components are filtered out before the export, to improve visibility in complex architectures.</li> </ul> filter.remove-component-versions <ul> <li>Type: Boolean</li> <li>Default: false</li> <li>Description: If <code>true</code>, the versions of the components are filtered out before the export, to reduce updates when components are often released.</li> </ul> filter.remove-components-by-names <ul> <li>Type: List of Strings</li> <li>Default: empty List</li> <li>Description: list of components that should be filtered out before the export (e.g. otel-collector-service).</li> </ul>"},{"location":"configuration/#configuring-the-export","title":"Configuring the export","text":"export.on-empty <ul> <li>Type: Boolean</li> <li>Default: false</li> <li>Description: If <code>true</code>, CLARA will export the recovered architecture using the enabled exporters, even if the architecture is completely empty.   This could be useful for debugging purposes.</li> </ul>"},{"location":"configuration/#exporter-graphviz-optional","title":"Exporter: GraphViz (optional)","text":"export.exporters.graphviz.enable <ul> <li>Type: Boolean</li> <li>Default: true</li> <li>Description: Simple way to disable this exporter without removing all of its associated configuration.</li> </ul> export.exporters.graphviz.output-type <ul> <li>Type: String (one of <code>BMP</code>, <code>DOT</code>, <code>GIF</code>, <code>JPG</code>, <code>JPEG</code>, <code>JSON</code>, <code>PDF</code>, <code>PNG</code>, <code>SVG</code>, <code>TIFF</code>)</li> <li>Description: Output format of the export. <code>SVG</code> is known to work well and in most situations the best choice.</li> </ul> export.exporters.graphviz.output-file <ul> <li>Type: String</li> <li>Description: The file location (absolute or relative path) of the GraphViz output.</li> <li>Example: <code>generated/architecture.svg</code></li> </ul>"},{"location":"configuration/#exporter-gropius-optional","title":"Exporter: Gropius (optional)","text":"export.exporters.gropius.enable <ul> <li>Type: Boolean</li> <li>Default: true</li> <li>Description: Simple way to disable this exporter without removing all of its associated configuration.</li> </ul> export.exporters.gropius.project-id <ul> <li>Type: String</li> <li>Description: The ID of the Gropius-project to export the recovered architecture to.</li> </ul> export.exporters.gropius.graphql-backend-url <ul> <li>Type: String (a valid URL)</li> <li>Description: The URL where CLARA can interact with the GraphQL-API of Gropius.</li> </ul> export.exporters.gropius.graphql-backend-authentication export.exporters.gropius.graphql-backend-authentication.authentication-url <ul> <li>Type: String (a valid URL)</li> <li>Description: The URL where CLARA can obtain an authentication token from the Gropius-backend via username and password.</li> </ul> export.exporters.gropius.graphql-backend-authentication.username <ul> <li>Type: String</li> <li>Description: The username for obtaining an authentication token.</li> </ul> export.exporters.gropius.graphql-backend-authentication.password <ul> <li>Type: String</li> <li>Description: The password for obtaining an authentication token.</li> </ul> export.exporters.gropius.graphql-backend-authentication.client-id <ul> <li>Type: String</li> <li>Description: The OAuth client ID for obtaining an authentication token.</li> </ul> export.exporters.gropius.component-handling <ul> <li>Type: String (one of <code>Delete</code> or <code>Modify</code>)</li> <li>Description: The mode how should be dealt with already existing components. Delete deletes and re-creates the old one, modify changes attributes.</li> </ul> export.exporters.gropius.export-libraries <ul> <li>Type: Boolean</li> <li>Default: true</li> <li>Description: Simple way to disable exporting libraries, as this is a data-intensive process and not always wanted.</li> </ul>"},{"location":"configuration/#a-full-example-config","title":"A full example config","text":"<pre><code>app:\n  log-config: true\n  block-after-finish: false\n\naggregation:\n  platforms:\n    kubernetes:\n      include-kube-namespaces: false\n      namespaces:\n        - abc\n        - xyz\n      aggregators:\n        kube-api:\n          enable: true\n        dns:\n          enable: true\n          logs-since-time: 2024-02-01T00:00:00Z\n        open-telemetry:\n          enable: true\n          listen-port: 7878\n          listen-duration: 45 minutes\n        syft-sbom:\n          enable: true\n          sbom-file-path: sbom/\n          use-stored-sbom-files: false\n\nmerge:\n  comparison-strategy: Equals\n  show-messaging-communications-directly: true\n\nfilter:\n  remove-component-endpoints: false\n  remove-components-by-names:\n    - otel-collector-service\n\nexport:\n  on-empty: false\n  exporters:\n    graphviz:\n      enable: true\n      output-type: SVG\n      output-file: generated/architecture.svg\n    gropius:\n      enable: true\n      project-id: aaaaaaaa-1111-bbbb-2222-cccccccccccc\n      graphql-backend-url: http://my.backend.com:8080/graphql\n      graphql-backend-authentication:\n        authentication-url: http://my.backend.com:3000/authenticate/oauth/xxxxxxxx-1111-yyyy-2222-zzzzzzzzzzzz/token\n        username: ${CLARA_GROPIUS_GRAPHQL_USERNAME}\n        password: ${CLARA_GROPIUS_GRAPHQL_PASSWORD}\n        client-id: ${CLARA_GROPIUS_GRAPHQL_CLIENT_ID}\n</code></pre>"},{"location":"export/","title":"Export","text":"<p>CLARA offers two different ways for exporting the aggregated Architecture:</p> <ul> <li>GraphViz to create a quick SVG export of the architecture.</li> <li>Gropius to create a dynamic Gropius representation of the architecture, exported and added to the configured Gropius project. For more details on Gropius visit the GitHub Page.</li> </ul>"},{"location":"export/gropius/","title":"Gropius Export","text":"<p>Gropius is an open-source cross-component issue management system for component-based architectures. In order to enable managing cross-component dependencies, users can model component-based software architectures in a Gropius project, e.g. via the API. For more details on Gropius visit the GitHub Page.</p> <p>For configuration options of the export please check out the configurations page.</p>"},{"location":"export/gropius/#data-model","title":"Data Model","text":"<p>The data model of Gropius consists of components which can be specified with templates as well as relations between those components, also configurable via templates. A component must have a component and a repository-URL in order to be added to a project, which resembles an architecture.</p> <p>CLARA components are mapped to Gropius-components like this: | CLARA Metamodel| Gropius Metamodel | |--------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------| | InternalComponent | Component | | \u00a0\u00a0\u00a0\u00a0InternalComponent.Name | \u00a0\u00a0\u00a0\u00a0Component.Name | | \u00a0\u00a0\u00a0\u00a0InternalComponent.IpAddress | \u00a0\u00a0\u00a0\u00a0Component.Description | | \u00a0\u00a0\u00a0\u00a0InternalComponent.Version | \u00a0\u00a0\u00a0\u00a0Component.ComponentVersion | | \u00a0\u00a0\u00a0\u00a0InternalComponent.Namespace | \u00a0\u00a0\u00a0\u00a0MISSING | | \u00a0\u00a0\u00a0\u00a0InternalComponent.Endpoints | \u00a0\u00a0\u00a0\u00a0MISSING (Note, that Gropius is capable of modeling interfaces, yet due to a lack of time this is not performed in the current work.) | | \u00a0\u00a0\u00a0\u00a0MISSING | \u00a0\u00a0\u00a0\u00a0Component.RepositoryURL (Example URL) | | \u00a0\u00a0\u00a0\u00a0InternalComponent.Type | \u00a0\u00a0\u00a0\u00a0Component.ComponentTemplate | | \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Type.Database| \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Database Temp. | | \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Type.Microservice | \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Microservice Temp. | | \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Type.Messaging | \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Messaging Temp.| | \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0null| \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Base Component Temp. | | \u00a0\u00a0\u00a0\u00a0InternalComponent.Libraries | \u00a0\u00a0\u00a0\u00a0Components | | \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Library.Version | \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Component.ComponentVersion | | \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Library.Name | \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Component.Name | | \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Library.Name | \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Component.Description | | \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0MISSING | \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Component.ComponentTemplate (Library Temp.) | | \u00a0\u00a0\u00a0\u00a0InternalComponent.Library | \u00a0\u00a0\u00a0\u00a0Relation | | \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0InternalComponent.Version | \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Relation.Start| | \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Library.Version | \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Relation.End | | \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0MISSING | \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Relation.RelationTemplate (Includes Temp.) | | ExternalComponent | Component | | \u00a0\u00a0\u00a0\u00a0ExternalComponent.Name | \u00a0\u00a0\u00a0\u00a0Component.Name | | \u00a0\u00a0\u00a0\u00a0ExternalComponent.Domain | \u00a0\u00a0\u00a0\u00a0Component.Description | | \u00a0\u00a0\u00a0\u00a0ExternalComponent.Type | \u00a0\u00a0\u00a0\u00a0Component.ComponentTemplate (Misc Temp.) | | \u00a0\u00a0\u00a0\u00a0ExternalComponent.Version | \u00a0\u00a0\u00a0\u00a0Component.ComponentVersion | | Communication | Relation | | \u00a0\u00a0\u00a0\u00a0Communication.Source.Version | \u00a0\u00a0\u00a0\u00a0Relation.Start | | \u00a0\u00a0\u00a0\u00a0Communication.Target.Version | \u00a0\u00a0\u00a0\u00a0Relation.End | | \u00a0\u00a0\u00a0\u00a0MISSING | \u00a0\u00a0\u00a0\u00a0Relation.RelationTemplate (Calls Temp.) |</p> <p>The Gropius GraphQL API is utilized by CLARA in order to export the recovered architectures into a Gropius project.</p>"},{"location":"export/gropius/#export-flow","title":"Export Flow","text":"<p>The export works sketched like this based on the respective configuration:</p> <ul> <li>for all components recovered by CLARA:</li> <li>delete or update component</li> <li>create or update component version</li> <li>add component version to project</li> <li>add relations for all components </li> </ul> <p>For all CRUD operations there are predefined GraphQL queries which are transformed into Kotlin Models using this GraphQl gradle plugin  and executed using this GraphQL Kotlin Spring client. The GraphQL queries are located in the clara-graphql directory.</p>"},{"location":"export/svg/","title":"SVG Export","text":"<p>The GraphViz SVG exporter exports a superficial representation of the architecture as displayed in the example image below.</p> <p>Legend:</p> <ul> <li>Arrow: Communication from source component to destination component</li> <li>Octagon: Component</li> <li>component-name</li> <li>component-ip-address</li> <li>component-endpoints</li> <li>Rectangle: External Component not in the namespace</li> <li>Outer Square: Kubernetes Namespace of the collected data</li> </ul> <p>For configuration options please check out the configurations page.</p>"},{"location":"filtering/","title":"Filtering","text":"<p>Filtering is applied as third step in the data pipeline, see concept. Filters can be added/removed by plug-and-play. For details see the configurations page.</p>"},{"location":"filtering/#filtering-options","title":"Filtering options","text":"<ul> <li>Remove Component Endpoints: The attribute \"Endpoints\" is filtered out (in order to make the SVG export more visually appealing). </li> <li>Remove Components by Name: Components can be filtered out if they are not directly part of the application (e.g. the OpenTelemetry Collector).</li> </ul>"},{"location":"merging/","title":"Merging","text":"<p>Merging is applied as third step in the data pipeline, as shown in concept. Merging is mandatory, as the results of the different aggregations need to be merged into a homogenous data format to retrieve a holistic picture.  Further, duplications are removed. For details on configuration possibilities see the configurations page.</p>"},{"location":"merging/#concept","title":"Concept","text":"<p>The following concepts and data operations are applied in the merging step of CLARA:  </p> <ul> <li>Base and Comparison Components </li> <li>Comparing </li> <li>Merging</li> <li>Dealing with Renamed Components </li> <li>Leftover Components </li> <li>Communications </li> <li>Adjusting Messaging Communications</li> </ul>"},{"location":"merging/#base-and-comparison-components","title":"Base and Comparison Components","text":"<p>In CLARA the merging of two detected components by different aggregators is defined as merging a comparison object on top of the base object.  In general, the components aggregated from the Kubernetes API are considered as the base component and OpenTelemetry components are considered as compare components. This is the case, because the Kubernetes API can be perceived as the ground truth about what is deployed in the cluster.</p>"},{"location":"merging/#comparing","title":"Comparing","text":"<p>In the comparison step for every Kubernetes component a matching OpenTelemetry component is searched. The matching is currently only be done by the name. Thereby CLARA can be configured to match only equal names or also match if one name contains the other (e.g. cart-pod-12345 and cart).</p>"},{"location":"merging/#merging_1","title":"Merging","text":"<p>In the merging step both component objects from Kubernetes and from OpenTelemetry are merged into a new final component object. Thereby, the Kubernetes component is providing the service-name, Kubernetes namespace, IP-address, and if applicable the version. The OpenTelemetry Component provides the endpoints and most likely the service type (e.g. database).</p>"},{"location":"merging/#dealing-with-renamed-components","title":"Dealing with Renamed Components","text":"<p>If a merged component was matched via a \"contains\"-pattern matching it is likely, that the final component has a different name then the OpenTelemetry component. Thus, the relations discovered between OpenTelemetry components need to be adjusted to match the new naming.</p>"},{"location":"merging/#leftover-components","title":"Leftover Components","text":"<p>All components that could not be matched are simply mapped to a final component, with whatever attributes are available, to not lose any information.</p>"},{"location":"merging/#communications","title":"Communications","text":"<p>Communications do not really have to be merged, as they are simply stacked upon each other in the exporter and do not contain any meta-information except source and target.</p>"},{"location":"merging/#adjusting-messaging-communications","title":"Adjusting Messaging Communications","text":"<p>Communications that are tagged as messaging communication, are also adjusted in the merger. CLARA can be configured to either show communications via a message broker or filter out the message broker and show the communications between the communications directly. The latter can make it easier to understand the real communication flows of an application, especially if everything runs via a message broker.</p>"},{"location":"setup/","title":"Setup instructions for CLARA step-by-step","text":"<p>These instructions will walk you through the initial installation and setup of the CLARA project. A deployed instance of the Gropius project as well as access to a kubernetes cluster are required.</p>"},{"location":"setup/#1-prerequisites","title":"1. Prerequisites","text":""},{"location":"setup/#11-getting-clara","title":"1.1. getting CLARA","text":"<ul> <li> <p>clone the CLARA repository</p> <p><pre><code>git clone https://github.com/ccims/clara.git\n</code></pre> or <pre><code>git clone git@github.com:ccims/clara.git\n</code></pre></p> </li> </ul> <p>Java Installation</p> <p>Make sure you have at least a Java 17 JVM installed and configured on your machine.</p>"},{"location":"setup/#12-kube-api","title":"1.2. kube-api","text":"<ul> <li>Ensure you have administrative rights for your Kubernetes cluster.</li> <li>Ensure you have configured the target namespace you want to analyze, in the context of your local kube-config.</li> </ul>"},{"location":"setup/#13-install-ktunnel","title":"1.3. Install ktunnel:","text":"<p>ktunnel allows CLARA to stream data from inside the cluster to the outside, thus not needing to be deployed inside the cluster.</p> <p>Either use homebrew: <pre><code>brew tap omrikiei/ktunnel &amp;&amp; brew install omrikiei/ktunnel/ktunnel\n</code></pre> or fetch the binaries from the release page.</p>"},{"location":"setup/#2-aggregator-setup-and-configuration","title":"2. Aggregator Setup and Configuration","text":"<p>CLARA relies on different aggregation components, that each need individual preparation. Although each aggregator is not mandatory, it is recommended to go through the setup of all following aggregators.</p>"},{"location":"setup/#21-opentelemetry-auto-instrumentation","title":"2.1 OpenTelemetry auto-instrumentation","text":"<p>CLARA utilizes the opentelemetry auto-instrumentation to add spans to the cluster's communication.</p> <ul> <li>Check if cert manager is installed in the cluster. If not run:     <pre><code>kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.14.4/cert-manager.yaml\n</code></pre></li> <li>Install the OpenTelemetry operator into the cluster:     <pre><code>kubectl apply -f https://github.com/open-telemetry/opentelemetry-operator/releases/latest/download/opentelemetry-operator.yaml\n</code></pre></li> <li>Install the OpenTelemetry collector into the target namespace:     <pre><code>kubectl apply -f &lt;path-to-clara&gt;/deployment/open-telemetry-collector/configmap.yml\nkubectl apply -f &lt;path-to-clara&gt;/deployment/open-telemetry-collector/deployment.yml \n</code></pre></li> <li>Add the instrumentation object into the target namespace:     <pre><code>kubectl apply -f &lt;path-to-clara&gt;/deployment/open-telemetry-collector/autoinstrumentation.yml\n</code></pre></li> <li>Instrument all existing deployments (that use below listed frameworks/technology) in the target namespace by configuring the <code>&lt;your-namespace&gt;.yaml</code>:     <pre><code>metadata:\n  annotations:\n    instrumentation.opentelemetry.io/inject-java: \"true\"\n    instrumentation.opentelemetry.io/inject-dotnet: \"true\" \n    instrumentation.opentelemetry.io/inject-go: \"true\" \n    instrumentation.opentelemetry.io/inject-nodejs: \"true\" \n    instrumentation.opentelemetry.io/inject-python: \"true\" \n</code></pre></li> </ul>"},{"location":"setup/#22-coredns","title":"2.2. CoreDNS","text":"<p>Ensure you can access and if necessary configure the kube-dns in the kube-system namespace. When using a managed cluster from a service provider, changes to core components of Kubernetes might not be allowed directly. Please consult the documentation of your respective provider.</p> <ul> <li>Ensure you can see the logs of your kube-dns component and it logs DNS requests by running:     <pre><code>kubectl logs -l k8s-app=kube-dns -n kube-system\n</code></pre></li> <li>Ensure you see logs of this format:     <pre><code>[INFO] 10.244.0.19:35065 - 3179 \"A IN kubernetes.default.svc.cluster.local.svc.cluster.local. udp 72 false 512\" NXDOMAIN qr,aa,rd 165 0.0000838s\n</code></pre></li> <li>If you don't see such logs, configure your kube-dns accordingly, based on your service-provider.</li> </ul>"},{"location":"setup/#23-install-anchoresyft","title":"2.3. Install anchore/syft","text":"<p>CLARA uses syft to generate SBOMs from container images.</p> <ul> <li>Install the binary from anchore/syft for your respective OS: macOS:     <pre><code>brew install syft\n</code></pre>     All OS:     <pre><code>curl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh | sh -s -- -b /usr/local/bin \n</code></pre></li> </ul>"},{"location":"setup/#3-run-clara","title":"3. Run CLARA","text":"<ul> <li>CLARA comes with a default config, yet it is recommended to check the configuration options and adjust them to your needs.    It is also recommended to make some dry runs with stripped down, minimal config to see if the configuration works properly.    For configuration options see: configurations page.    The config file of CLARA can be found at <code>&lt;path-to-clara&gt;/clara-app/src/main/resources/config.yml</code></li> <li>In this <code>config.yml</code> please insert your specific URLs and authorization information for accessing your deployed Gropius instance.    Sensitive credentials are prepared to be set as environment variables.</li> <li>To build CLARA run in the clara dictionary:     <pre><code>./gradlew clean build standaloneJar\n</code></pre></li> <li>If you want to use the Gropius exporter, set the Gropius environment variables:     <pre><code>export CLARA_GROPIUS_GRAPHQL_CLIENT_ID=&lt;your-id&gt;\nexport CLARA_GROPIUS_GRAPHQL_PASSWORD=&lt;your-password&gt;\nexport CLARA_GROPIUS_GRAPHQL_USERNAME=&lt;your-username&gt;\n</code></pre></li> <li>Start CLARA by executing the application:     <pre><code>java -jar clara-app/build/libs/clara-app-*.jar\n</code></pre></li> <li>Run ktunnel:      <pre><code>ktunnel inject deployment otel-collector-deployment 7878 -n &lt;your-namespace&gt;\n</code></pre></li> </ul>"},{"location":"validation/t2-reference-architecture/","title":"T2-Reference-Architecture","text":"<p>CLARA will be evaluated against the T2-Microservice-Reference-Architecture.</p>"},{"location":"validation/t2-reference-architecture/#setup","title":"Setup","text":"<p>Follow the deployment instructions from the documentation. </p>"}]}